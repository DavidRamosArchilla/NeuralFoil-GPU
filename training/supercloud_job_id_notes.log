- 23263437: 4 layers,  64 wide, tanh,  1e-5 LR. AdamW.   BS 64. Fresh start. Train Loss asymptote 0.073 @ Epoch 500. Killed.
- 23264921: 4 layers,  64 wide, tanh,  1e-6 LR. AdamW.   BS 64. Restarted from 23263437. No benefit, LR too low. Killed.
- 23264949: 6 layers,  64 wide, tanh,  1e-4 LR. AdamW.   BS 64. Fresh start. Train Loss asymptote 0.077 @ Epoch  93. Killed.
- 23265409: 4 layers, 128 wide, tanh,  1e-4 LR. AdamW.   BS 64. Fresh start. Train Loss asymptote 0.053 @ Epoch 140. Killed.
- 23265656: 4 layers, 256 wide, tanh,  1e-4 LR. AdamW.   BS 64. Fresh start. Train Loss asymptote 0.042 @ Epoch  80. Killed.
- 23265690: 4 layers, 256 wide, tanh,  8e-4 LR. AdamW.  BS 512. Fresh start. Train Loss asymptote 0.025 @ Epoch 250. Killed.
- 23265746: 4 layers, 256 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. Train Loss asymptote 0.020 @ Epoch 800. Killed.
- 23265990: 4 layers, 256 wide, tanh, 32e-4 LR. RAdam. BS 2048. Fresh start. Train Loss asymptote 0.021 @ Epoch 700. Killed
- 23266008: 4 layers,  64 wide, tanh, 32e-4 LR. RAdam. BS 2048. Fresh start. Train Loss asymptote 0.048 @ Epoch 100. Killed.
- 23266260: 4 layers, 256 wide, spls,  8e-4 LR. RAdam.  BS 512. Fresh start. Train Loss asymptote 0.020 @ Epoch 600. Killed.

New log format:

- 23266380: 4 layers, 256 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Test loss 0.02155 @ Epoch 500. Killed.
- 23266454: 4 layers, 256 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on test loss plateau. Test loss 0.05853 @ Epoch 400. Killed.
- 23266738: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on test loss plateau. Test loss 0.03567 @ Epoch 200. Killed.
- 23266740: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on test loss plateau. Dropout 0.5 (from 0.2). Test loss 0.05651 @ Epoch 160. Killed.

- 23266893: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Test loss 0.04209 @ Epoch 900. Killed.
- 23266894: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0.1 (from 0.2). Test loss 0.04156 @ Epoch 900. Killed.
- 23266896: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0.1 (from 0.2). Weight decay 1e-4. Test loss 0.02392 @ Epoch 900. Killed.
- 23266897: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0.1 (from 0.2). Weight decay 1e-2. Test loss 0.13329 @ Epoch 116. Killed.

- 23266906: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0.1 (from 0.2). Test loss 0.03982 @ Epoch 900. Killed.
- 23266914: 4 layers,  64 wide, tanh,  8e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Test loss 0.01777 @ Epoch 900. Killed.
- 23266999: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-4. Test loss 0.01379 @ Epoch 800. Killed.

- 23267000: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-5. Test loss 0.00991 @ Epoch 1200. Killed.
- 23267001: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-6. Test loss 0.00956 @ Epoch 1200. Killed.

- 23267375: 4 layers,  32 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-5. Test loss 0.01349 @ Epoch 1600. Killed.
- 23267376: 4 layers,  32 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-6. Test loss 0.01299 @ Epoch 1600. Killed.

- 23267378: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-5. Batch norms before activations (from after). Test loss 0.01138 @ Epoch 1600. Killed.
- 23267379: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-6. Batch norms before activations (from after). Test loss 0.01102 @ Epoch 1600. Killed.

- 23267380: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-5. No batch norm. Test loss 0.00862 @ Epoch 1600. Killed.
- 23267381: 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-6. No batch norm. Test loss 0.00876 @ Epoch 1600. Killed.

- 23267841: 4 layers,  48 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-5. No batch norm. Test loss 0.01071 @ Epoch 1300. Killed.

- 23267842: 4 layers,  32 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Dropout 0. (from 0.2). Weight decay 1e-5. No batch norm. Test loss 0.01312 @ Epoch 1200. Killed.

Baselines going forward:
          - 4 layers,  64 wide, tanh,  1e-4 LR. RAdam.  BS 512. Fresh start. LR schedule on train loss plateau. Weight decay 1e-5. No batch norm or dropout.


- 23272184: Mislabeled, lost. 3 layers, 32 wide. Test loss 0.01305
- 23272186: 4 layers, 32 wide. Test loss 0.01345
- 23272188: 4 layers, 48 wide. Test loss 0.01065
- 23272190: 4 layers, 64 wide. Test loss 0.00967
- 23272191: 4 layers, 128 wide. Test loss 0.00834

- 23272198: 6 layers, 32 wide. Test loss 0.01079
- 23272199: 6 layers, 16 wide. Test loss 0.01680

Recommended final model for minimum test loss:

xxsmall: 2 layers,  32 wide. 23277974.
xsmall:  3 layers,  32 wide. 23277977.
small:   3 layers,  48 wide. 23277979.
medium:  4 layers,  64 wide. 23277981.
large:   4 layers, 128 wide. 23277983.
xlarge:  5 layers, 256 wide. 23277984.
xxlarge: 5 layers, 512 wide. 23277987.

Results: Found significant errors when analyzing out-of-sample airfoils. Need to make some changes:
    a) Need more data beyond the UIUC two-combination airfoils. Added 3-combination + variance from random deviation.
    b) Net training / evaluation needs to embed the invariant of symmetry w.r.t. alpha and shape, targeting symmetric / antisymmetric outputs.

xxsmall: 2 layers,  32 wide. 23296397.
xsmall:  3 layers,  32 wide. 23296400.
small:   3 layers,  48 wide. 23296405.
medium:  4 layers,  64 wide. 23296406.
large:   4 layers, 128 wide. 23296413.
xlarge:  4 layers, 256 wide. 23296435.
xxlarge: 5 layers, 256 wide. 23296439.
xxxlarge:5 layers, 512 wide. 23296446.